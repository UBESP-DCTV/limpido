library(limpido)
library(keras)

# Parameters ======================================================
params <- setup_input_data(
    validation_len = 300L,
    max_words = Inf,            # ~20% of the full dictionary (Pareto)
    embedding_dim  = "300",
    maxlen = 500L,
    data_path   = here::here("../data/"),
    output_path = here::here("../output/"),
    random_seed = 1234L, # sample.int(1e4, 1), #
    mixdb_name  = "mixdb_otiti_tagged.rds",
    verbose = TRUE,
    batch_size = 8L, #16L, #
    epochs = 300L,

    loss      = "categorical_crossentropy",
    metrics   = "categorical_accuracy",
    optimizer = "adam",
    is_test   = FALSE
)


input <- layer_input(shape = c(params$maxlen), name = "input")

embedding <- input %>%
  layer_embedding(
        input_dim = params$max_words,
        output_dim = params$embedding_dim,
        input_length = params$maxlen,
        trainable = FALSE,
        weights = params$embedding_matrix,
        name = "1gram_embedding"
    )

model <- keras_model(input, embedding)

all.equal(
    params$embedding_matrix[[1L]][12L, ],
    predict(model, matrix(rep(11L, 500L), nrow = 1L))[1L, 1L, ]
)


#> fasttext_pretrained %>% stringr::str_subset("^otite ")

#[1] "otite 0.085886 0.077341 0.10098 -0.26065 0.69038 -0.0025183
#-0.12611 -0.072631 -0.053026 0.33478 0.1646 0.43538 0.11546 0.17189
#-0.11375 -0.15203 0.35294 0.23446 -0.18534 -0.022319 0.18529 -0.47113
#0.25102 -0.37373 -0.95351 0.46278 0.43389 0.56842 0.35977 -0.3563
#-0.18766 -0.46883 0.39846 0.18246 -0.19922 0.11182 0.081488 0.36485
#0.12795 -0.099057 -0.057628 -0.14812 -0.11986 0.038223 -0.59139 0.17856
#0.24153 0.059211 -0.069998 0.56019 -0.39514 -0.60656 -0.041523 0.12488
#-0.12697 -0.18577 0.01299 0.43587 0.22657 0.013672 -0.098006 0.14662
#-0.024194 0.0066201 -0.17152 0.3156 -0.29185 -0.73648 -0.02226 0.26075
#-0.16086 0.070327 -0.18602 0.14569 0.3061 0.11596 -0.11864 0.073245
#0.066091 0.40567 0.067987 0.16551 -0.025274 0.60412 -0.11192 0.36699
#-0.19743 -0.59197 -0.54442 0.14233 -0.21715 -0.19454 -0.49434 -0.30239
#0.15769 0.72063 0.0037437 0.077245 -0.13825 -0.71156 0.091912 -0.17682
#-0.033827 0.21726 -0.081597 0.03407 0.1064 -0.46316 -0.068802 0.66899
#0.036326 0.10981 -0.27978 0.56003 -0.62874 0.21896 -0.09169 -0.03445
#-0.17997 -0.43792 0.12066 0.18831 -0.014557 -0.45327 0.4631 0.42078
#0.25809 -0.41518 -0.55831 -0.045263 0.30071 0.034073 0.22444 0.11215
#0.17018 -0.052232 -0.043816 -0.18155 -0.098903 0.4547 -0.014829
#-0.20276 -0.10636 -0.37031 -0.040471 -0.26081 -0.5333 0.0098316
#-0.49494 0.13108 0.02669 0.6074 0.24477 0.55581 -0.18603 -0.11894
#-0.36639 -0.11532 -0.25288 0.31753 -0.29643 0.44459 0.25589 -0.0084723
#-0.24546 0.24931 -0.33078 0.11582 0.0079675 0.16396 0.22468 0.83955
#0.25987 -0.38264 -0.47361 0.13209 -0.060691 0.034741 0.0027408 0.35495
#0.3298 0.69426 0.38615 -0.019515 0.12145 -0.23565 0.22557 -0.4724
#0.025702 0.46307 0.0026493 -0.032048 -0.23558 0.21962 0.097478 0.32643
#-0.15084 0.14301 0.317 -0.49992 0.63709 0.14983 -0.4596 0.46111
#-0.61722 0.02548 -0.23377 0.4915 0.059855 -0.019208 -0.10753 -0.11577
#0.15921 -0.44093 -0.088082 -0.11324 -0.12565 -0.2633 0.13878 0.38325
#0.11791 0.31465 0.10528 -0.16025 -0.16664 0.10356 -0.3468 -0.11931
#0.0042363 -0.065932 0.26528 0.88403 0.35243 -0.033654 0.076133 -0.06783
#-0.03597 -0.27055 0.18655 0.01979 -0.55201 -0.2085 -0.17757 -0.097694
#0.10743 -0.070269 1.0962 0.26445 -0.34608 0.044804 -0.077047 0.044811
#0.10381 0.4458 0.063342 -0.5414 0.29198 -0.030626 -0.2797 0.15639
#0.70226 0.48542 -0.23 -0.39078 0.22988 0.16833 -0.033334 0.21511
#-0.28526 0.64038 -0.757 -0.0952 -0.015966 0.16752 0.21276 0.084618
#0.39725 0.23894 0.34275 0.049771 0.074697 -0.014811 -0.28021 0.24441
#0.31916 -0.17603 -0.22081 -0.37791 0.24091 0.54862 0.35784 0.45555
#-0.06325 0.21551 0.1544 -0.5123 -0.38097 0.25438 0.65352 -0.6204 "
