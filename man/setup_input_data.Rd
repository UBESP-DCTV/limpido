% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/setup_input_data.R
\name{setup_input_data}
\alias{setup_input_data}
\title{Setup for DL}
\usage{
setup_input_data(validation_len = 300L, max_words = Inf,
  embedding_dim = c("300", "100"), maxlen = 300L, batch_size = 8L,
  epochs = 15L, data_path = here::here("../../data/"),
  output_path = here::here("../../output/"),
  random_seed = sample.int(10000, 1),
  mixdb_name = "mixdb_otiti_tagged.rds", verbose = TRUE,
  loss = "categorical_crossentropy", metrics = "categorical_accuracy",
  optimizer = "adam")
}
\arguments{
\item{validation_len}{(int, default = 300L) Number of example in the
validation set (see Datails)}

\item{max_words}{(int, default = Inf) Maximum number from the
vocabulary to considera (Inf means all possible, i.e. the minimum
between the number of words in the corpus and the number of words
in the vocabulary, plus 1 for the "out of vocabulary" words)}

\item{embedding_dim}{(int, default = 300L) Dimension of the embedding
vectors (see Details)}

\item{maxlen}{(chr, defaul = 300L) Maximum number of words to
considered for each recrods (see Details)}

\item{batch_size}{(int, default = 8L) number of samples passed to the
learner each full step of learning.}

\item{epochs}{(int, default = 15L) At each step of learning a
progressive \code{batch_size} of samples of the whole are used in by
the learner. Every now the learner complete to see all the sample
in the training data, they said it is passed one epoch. \code{epochs}
is the number of time the learner will see the full trainng
dataset for its training.}

\item{data_path}{(chr) path to the folder in which find the data
needed for the setup}

\item{output_path}{(chr) path to the folder in which to save to
output objects}

\item{random_seed}{(int, default is random) seed for the random
computation}

\item{mixdb_name}{(chr) name (within \code{data_path} to the RDS file
containing \link{mixdb} with the training and the validation data.}

\item{verbose}{(lgl, default TRUE) should info on the progress
displayed?}

\item{loss}{(chr, default "categorical_crossentropy") the loss
function for the model}

\item{metrics}{(chr, default "categorical_accuracy") the metrics to
estimate the performance}

\item{optimizer}{(chr, dafault "adam") the optimizer for the DL model}
}
\value{
a named list including:
\itemize{
\item \strong{train_x}: list of named integers representig the training set
\item \strong{validation_x}: list of named integers representig the
validation set
\item \strong{train_y}: response variable for training (+ train-validation)
set
\item \strong{validation_y}: response variable for validation
(- train-validation) set
\item \strong{embedding_matrix}: a list containing the embedding matrix
(\code{max_words} + 1 rows, \code{embedding_dim} columns)
\item \strong{random_seed}: the seed used,
\item \strong{n_class}: the number of classes,
\item \strong{mean_train_len}: the mean length of a training document,
\item \strong{mean_validation_len}: the mean length of a validation
document,
\item \strong{train_len}: number of observation in the trainin gset (note:
the number of observation in the validation set is one of the
paramenter passed to the function)
\item values of all the imput parameters
}
}
\description{
Construnct all the necesserary stuff for deep learning with Keras
and TensorFlow.
}
\details{
\preformatted{The validation set should come from the data distribution of
the test set. The test is unseen but the validation set can be
(and would) be useful for the tuning of the hyper parameter,
i.e., to choose between different models. If the validation set
has a considerable dimension it is not necessary to use all of
it only for the validation but it could be usefull to incorporate
a portion into the training set (AKA the train-validation set).
The parameter `validation_len` allow to decide how meny
observation should be kept for the proper validation only. All
the observation in the given validation set that (randomly)
exceede `validation_len` will be incorporate in the training set.

Embedding vectors are dense representation of the feature. Those
vector can have been trained with different lengths. Pedianet
DB has two pretrained embeddings produced with the
FastText-skipgram procedure, one with vectors of length 100, and
one with vectors of length 300 (see [pedianet_fasttext]).

Neural network have to receive an imput of known an dfixed
dimension. `maxlen` define that dimension, i.e., every
observation which has more then `maxlen` words will be truncated,
and every observation which has less then `maxlen` words will be
padded with 0s down or up to `maxlen` length.
}
}
